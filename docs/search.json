[
  {
    "objectID": "M2/exercices-distribution_of_random_variables.html",
    "href": "M2/exercices-distribution_of_random_variables.html",
    "title": "Distributions of random variables",
    "section": "",
    "text": "For the exercices of this page, you need to add the packages Distributions.jl, LaTeXStrings.jl, Plots.jl and Test.jl. To install them please open Julia’s interactive session (known as REPL) and press ] key in the REPL to use the package mode, then add the packages:\njulia&gt; ]\npkg&gt; add Distributions\npkg&gt; add LaTeXStrings\npkg&gt; add Plots\npkg&gt; add Test\nWhen the installation is complete, import them:\n\nusing Distributions\nusing LaTeXStrings # just for labels in some plots\nusing Plots\nusing Test\n\nThe package Test.jl is used to test if the outputs of functions to complete are correct. The packages Plots.jl is for data visualization. Almost everything in Plots is done by specifying plot attributes. Do not hesitate to have a look to this tutorial. Finally, the package Distributions.jl provides a large collection of probabilistic distributions and related functions.",
    "crumbs": [
      "Master 2",
      "Tutorials",
      "Distribution of random variables"
    ]
  },
  {
    "objectID": "M2/exercices-distribution_of_random_variables.html#preliminaries",
    "href": "M2/exercices-distribution_of_random_variables.html#preliminaries",
    "title": "Distributions of random variables",
    "section": "",
    "text": "For the exercices of this page, you need to add the packages Distributions.jl, LaTeXStrings.jl, Plots.jl and Test.jl. To install them please open Julia’s interactive session (known as REPL) and press ] key in the REPL to use the package mode, then add the packages:\njulia&gt; ]\npkg&gt; add Distributions\npkg&gt; add LaTeXStrings\npkg&gt; add Plots\npkg&gt; add Test\nWhen the installation is complete, import them:\n\nusing Distributions\nusing LaTeXStrings # just for labels in some plots\nusing Plots\nusing Test\n\nThe package Test.jl is used to test if the outputs of functions to complete are correct. The packages Plots.jl is for data visualization. Almost everything in Plots is done by specifying plot attributes. Do not hesitate to have a look to this tutorial. Finally, the package Distributions.jl provides a large collection of probabilistic distributions and related functions.",
    "crumbs": [
      "Master 2",
      "Tutorials",
      "Distribution of random variables"
    ]
  },
  {
    "objectID": "M2/exercices-distribution_of_random_variables.html#empirical-distribution-function",
    "href": "M2/exercices-distribution_of_random_variables.html#empirical-distribution-function",
    "title": "Distributions of random variables",
    "section": "Empirical distribution function",
    "text": "Empirical distribution function\n\nExercise 1\n\nBuild the empirical cumulative distribution function, also called eCDF.\n\n\n\n\n\n\n\nNote\n\n\n\nUse broadcasting to complete the following code.\n\na = [1,2,3.5]\na .&lt; 2\n\n3-element BitVector:\n 1\n 0\n 0\n\n\n\n\n\n\"\"\"\n   Compute de number of element in the vactor t less than a value x\n   input\n   t : Vector of Real\n   x : Real\n   Output\n   Integer\n\"\"\"\nfunction empirique(t::Vector{&lt;:Real}, x::Real)::Int\n    # to complete\n    return sum(t .&lt; x)  # .&lt; vectorial operation\nend\n\n\nprintln(\"empirique([1.,2,3],1.5) = \", empirique([1.,2,3],1.5))\n\nTest.@test empirique([1.,2,3],1.5) == 1\n\nempirique([1.,2,3],1.5) = 1\n\n\n\nTest Passed\n\n\n\n\n# If the type of the vector elements is not a real then there is an error\nprintln(\"empirique([1.+2im,2,3],1.5) = \", empirique([1.,2+2im,3],1.5))\n\n\nMethodError: no method matching empirique(::Vector{ComplexF64}, ::Float64)\nThe function `empirique` exists, but no method is defined for this combination of argument types.\n\nClosest candidates are:\n  empirique(::Vector{&lt;:Real}, ::Real)\n   @ Main In[352]:9\n\n\nStacktrace:\n [1] top-level scope\n   @ In[353]:2\n\n\n\n\nGenerate a sample of N=1000 datas from a uniform distribution on [0,2] and plot the eCDF of this sample.\n\n\nN = 1000 # number of datas\nu = 2*rand(N)   # uniform law on [0,2]\nx_grid = -1:0.1:3\n\n# Plot of the empirical cumulative distribution function\nF(x) = empirique(u,x)/N\np_uniform_cdf = plot(x_grid,F,xlabel=\"x\", ylabel=\"F(x)\", legend=false)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdd on the plot the eCDF with the Distribution.jl package.",
    "crumbs": [
      "Master 2",
      "Tutorials",
      "Distribution of random variables"
    ]
  },
  {
    "objectID": "M2/exercices-distribution_of_random_variables.html#distributions.jl-package",
    "href": "M2/exercices-distribution_of_random_variables.html#distributions.jl-package",
    "title": "Distributions of random variables",
    "section": "Distributions.jl Package",
    "text": "Distributions.jl Package\n\nIntroduction\nThere is lots of libraries (packages in julia): https://julialang.org/packages/. For the documentation of the Distributions Package see https://juliastats.org/Distributions.jl/stable/.\n\na = 0; b = 2;\ndist = Uniform(a,b)  # dist is an object : the uniform distribution on [a,b]\nprintln(\"type de dist = \",typeof(dist))\n# you can acces to the mean or median of the distribution\nprintln(\"mean(dist) = \", mean(dist))\nprintln(\"median(dist) = \", median(dist))\n# and the the PDF, CDF and inverse CDF function of the distribution\nprintln(\"pdf(1.2) = \", pdf(dist,1.2))\nprintln(\"pdf(3) = \", pdf(dist,3))\nprintln(\"cdf(1.2) = \", cdf(dist,1.2))\nprintln(\"cdf(3) = \", cdf(dist,3))\nprintln(\"inverse of cdf(0.75) = \", quantile(dist,0.75))\n\ntype de dist = Uniform{Float64}\nmean(dist) = 1.0\nmedian(dist) = 1.0\npdf(1.2) = 0.5\npdf(3) = 0.0\ncdf(1.2) = 0.6\ncdf(3) = 1.0\ninverse of cdf(0.75) = 1.5\n\n\n\n\nExercise 2\nPlot on the same first graph the CFD of the uniform distribution on [0,2]\n\ncdf_uniform(x) = cdf(dist,x)\nplot!(p_uniform_cdf,x_grid,cdf_uniform,xlabel=\"x\", ylabel=\"F(x)\", legend=false)",
    "crumbs": [
      "Master 2",
      "Tutorials",
      "Distribution of random variables"
    ]
  },
  {
    "objectID": "M2/exercices-distribution_of_random_variables.html#triangular-distribution",
    "href": "M2/exercices-distribution_of_random_variables.html#triangular-distribution",
    "title": "Distributions of random variables",
    "section": "Triangular Distribution",
    "text": "Triangular Distribution\nWe consider the distribution with the following density distribution\nf(x) = \\begin{cases}\nx\\quad\\textrm{pour}\\quad x\\in[0,1]\\\\\n2-x\\quad\\textrm{pour}\\quad x\\in[1,2]\\\\\n0\\quad\\textrm{sinon}\n\\end{cases}\nPlot the density, cumulative dendity and inverse cumulative function.\n\na = 0; b = 2;\ndist = TriangularDist(a,b,1)  # min = a; max = b; mode = 1\nprintln(\"type de dist = \",typeof(dist))\nprintln(\"params(dist) = \", params(dist))\n\n# Density function\np1 = plot(x_grid, x-&gt;pdf(dist,x), color = :blue, linewidth=2, xlabel=(L\"x\"), ylabel=(L\"f(x)\")) \n# Cumulative density function\np2 = plot(a-1:0.01:b+1, x-&gt;cdf(dist,x), linewidth=2, xlabel=(L\"x\"), ylabel=(L\"F(x)\"))  \n# Inverse cumulative density function\np3 = plot(0:0.01:1, x-&gt;quantile(dist,x), xlims=(0,1), ylims=(0,2), color = :green, linewidth=2, xlabel=(L\"u\"), ylabel=(L\"F^{-1}(u)\"))\nplot(p1,p2,p3, layout=(1,3),legend = false,size = (1200,300), margin = 0.6Plots.cm)\n\ntype de dist = TriangularDist{Float64}\nparams(dist) = (0.0, 2.0, 1.0)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHistogram\nGenerate a sample of 100 datas from the triangular distribution and plot on the same graph the histogram of the simple and the PDF function\n\n# Sample of 100 datas\nt = rand(dist,100)\nhistogram(t) \nhistogram(t,normalize=true) # normalize=true pour ajouter la fonction de densité\nplot!(a-0.5:0.1:b+0.5, x-&gt;pdf.(dist,x), linewidth=2, xlabel=(L\"x\"), ylabel=(L\"f(x)\"))   \n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\nWhat is the problem ?\n\nUse the normalize=true parameter in the histogram function for solving the problem\nExecute for a sample of N = 10000 datas",
    "crumbs": [
      "Master 2",
      "Tutorials",
      "Distribution of random variables"
    ]
  },
  {
    "objectID": "M2/exercices-distribution_of_random_variables.html#example-of-discret-distribution",
    "href": "M2/exercices-distribution_of_random_variables.html#example-of-discret-distribution",
    "title": "Distributions of random variables",
    "section": "Example of discret distribution",
    "text": "Example of discret distribution\nWe present here the binomial distribution which is a discrete probability distribution.\n\nn, p, N = 10, 0.2, 10^3\nbDist = Binomial(n,p)\nxgrid = 0:n\nplot(xgrid,pdf.(bDist,xgrid), color=:orange, seriestype = :scatter)\nplot!(xgrid,pdf.(bDist,xgrid), line = :stem, linewidth=2, color=:orange)",
    "crumbs": [
      "Master 2",
      "Tutorials",
      "Distribution of random variables"
    ]
  },
  {
    "objectID": "M2/exercices-distribution_of_random_variables.html#central-limit-theorem",
    "href": "M2/exercices-distribution_of_random_variables.html#central-limit-theorem",
    "title": "Distributions of random variables",
    "section": "Central limit theorem",
    "text": "Central limit theorem\nWe are going to illustrate the central limit theorem :\nSuppose X_1,X_2,\\ldots is a sequence of Independent and identically distributed random variables with E(X_i)=\\mu and Var(X_i)=\\sigma^2 &lt; +\\infty. Then, as n approaches infinity, the random variables \\sqrt{n}(\\bar{X}_n - \\mu) converge in distribution to a normal distribution \\mathcal{N}(0,\\sigma^2)\n\nExercise\n\nChoose a distribution law dist, compute its mean \\mu and its variance \\sigma^2 and N the number of sanple\nFor n in (1,2,5,20)\n\nGenerate N=10000 samples of lenght n from the dist distribution\nCompute the means of the N samples and the N values \\sqrt{n}(\\bar{X}_n - \\mu)\nPlot the histogram of these N values and the normal distribution \\mathcal{N}(0,\\sigma^2)\n\n\n\nX = rand(dist,(2,3))\nprintln(X)\n(mean(X,dims=1))\n\n[0.3900501110792506 1.6017523790079928 0.8608969107894937; 0.8100903551087728 0.820605938432214 0.7620181237169289]\n\n\n1×3 Matrix{Float64}:\n 0.60007  1.21118  0.811458\n\n\n\ndist = Uniform(0,12)\nμ = mean(dist)\nσ = std(dist)\nnormal_dist = Normal(0,σ)\nprintln(L\"\\mu = \", μ)\nprintln(L\"\\sigma = \", σ)\nN = 10000\nn_mean = (1,2,5,20)\n\np = []\nfor n in n_mean \n    X = rand(dist,(n,N))\n    Xbar_n = vec(sqrt(n)*(mean(X,dims=1) .- μ)) # to have a vector and not a matrix of size (1,3)\n    \n    p1 = histogram(Xbar_n,normalize=true) # normalize=true pour ajouter la fonction de densité\n    plot!(p1, -3*σ:0.1:3*σ, x-&gt;pdf.(normal_dist,x), linewidth=2, xlabel=(L\"x\"), ylabel=(L\"f(x)\"))   \n    push!(p,p1)\nend\nprintln(p)\nplot(p[1],p[2],p[3],p[4],legend = false)\n\n$\\mu = $6.0\n$\\sigma = $3.4641016151377544\nAny[Plot{Plots.GRBackend() n=2}, Plot{Plots.GRBackend() n=2}, Plot{Plots.GRBackend() n=2}, Plot{Plots.GRBackend() n=2}]",
    "crumbs": [
      "Master 2",
      "Tutorials",
      "Distribution of random variables"
    ]
  },
  {
    "objectID": "M2/resources/TP1-distributions.html",
    "href": "M2/resources/TP1-distributions.html",
    "title": "Introduction to julia for statistic",
    "section": "",
    "text": "To contact me, here is my email : joseph.gergaud@toulouse-inp.fr\nThe files ares here :\nhttps://gitlab.irit.fr/toc/etu-n7/julia\ndirectory M2"
  },
  {
    "objectID": "M2/resources/TP1-distributions.html#empirical-cumulative-distribution-function-ecdf",
    "href": "M2/resources/TP1-distributions.html#empirical-cumulative-distribution-function-ecdf",
    "title": "Introduction to julia for statistic",
    "section": "empirical cumulative distribution function, eCDF",
    "text": "empirical cumulative distribution function, eCDF\n\nExercise 1\n\nBuild the empirical cumulative distribution function\n\n\n# use broadcasting\na = [1,2,3.5]\na .&lt; 2\n\n3-element BitVector:\n 1\n 0\n 0\n\n\n\nusing Test   # for tests\n\n\"\"\"\n   Compute de number of element in the vactor t less than a value x\n   input\n   t : Vector of Real\n   x : Real\n   Output\n   Integer\n\"\"\"\nfunction empirique(t::Vector{&lt;:Real}, x::Real)::Int\n    # to complete\n    return sum(t .&lt; x)  # .&lt; vectorial operation\nend\n\n\nprintln(\"empirique([1.,2,3],1.5) = \", empirique([1.,2,3],1.5))\n\nTest.@test empirique([1.,2,3],1.5) == 1\n\nempirique([1.,2,3],1.5) = 1\n\n\n\nTest Passed\n\n\n\n\n# If the type of the vector elements is not a real then there is an error\nprintln(\"empirique([1.+2im,2,3],1.5) = \", empirique([1.,2+2im,3],1.5))\n\nMethodError: MethodError: no method matching empirique(::Vector{ComplexF64}, ::Float64)\nThe function `empirique` exists, but no method is defined for this combination of argument types.\n\nClosest candidates are:\n  empirique(!Matched::Vector{&lt;:Real}, ::Real)\n   @ Main ~/Courses/julia/course-tse-julia/M2/resources/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W3sZmlsZQ==.jl:11\n\nMethodError: no method matching empirique(::Vector{ComplexF64}, ::Float64)\n\nThe function `empirique` exists, but no method is defined for this combination of argument types.\n\n\n\nClosest candidates are:\n\n  empirique(!Matched::Vector{&lt;:Real}, ::Real)\n\n   @ Main ~/Courses/julia/course-tse-julia/M2/resources/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W3sZmlsZQ==.jl:11\n\n\n\n\n\nStacktrace:\n\n [1] top-level scope\n\n   @ ~/Courses/julia/course-tse-julia/M2/resources/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W4sZmlsZQ==.jl:2\n\n\n\nGenerate a sample of N=1000 datas from a uniform distribution on [0,2] and plot the eCDF of this sample\n\n\nusing Plots  # for plots\nN = 1000 # number of datas\nu = 2*rand(N)   # uniform law on [0,2]\nx_grid = -1:0.1:3\n# Plot of the empirical cumulative distribution function\nusing Plots\nF(x) = empirique(u,x)/N\np_uniform_cdf = plot(x_grid,F,xlabel=\"x\", ylabel=\"F(x)\", legend=false)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdd on the plot the Cumulative Distribution Function For thie use de Distributions Package\n\n\n# add the cumulative distribution function\nusing Distributions"
  },
  {
    "objectID": "M2/resources/TP1-distributions.html#distributions-package",
    "href": "M2/resources/TP1-distributions.html#distributions-package",
    "title": "Introduction to julia for statistic",
    "section": "Distributions Package",
    "text": "Distributions Package\n\nIntroduction\nThere is lots of libraries (Packages in julia) : https://julialang.org/packages/\nFor the documentation of the Distributions Package see\nhttps://juliastats.org/Distributions.jl/stable/\n\nusing Distributions\nusing Plots\nusing LaTeXStrings\na = 0; b = 2;\ndist = Uniform(a,b)  # dist is an object : the uniform distribution on [a,b]\nprintln(\"type de dist = \",typeof(dist))\n# you can acces to the mean or median of the distribution\nprintln(\"mean(dist) = \", mean(dist))\nprintln(\"median(dist) = \", median(dist))\n# and the the PDF, CDF and inverse CDF function of the distribution\nprintln(\"pdf(1.2) = \", pdf(dist,1.2))\nprintln(\"pdf(3) = \", pdf(dist,3))\nprintln(\"cdf(1.2) = \", cdf(dist,1.2))\nprintln(\"cdf(3) = \", cdf(dist,3))\nprintln(\"inverse of cdf(0.75) = \", quantile(dist,0.75))\n\ntype de dist = Uniform{Float64}\nmean(dist) = 1.0\nmedian(dist) = 1.0\npdf(1.2) = 0.5\npdf(3) = 0.0\ncdf(1.2) = 0.6\ncdf(3) = 1.0\ninverse of cdf(0.75) = 1.5\n\n\n\nExercise 2\nPlot on the same first graph the CFD of the uniform distribution on [0,2]\n\ncdf_uniform(x) = cdf(dist,x)\nplot!(p_uniform_cdf,x_grid,cdf_uniform,xlabel=\"x\", ylabel=\"F(x)\", legend=false)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTriangular Distribution\nWe consider the distribution with the following density distribution f(x) = \\begin{cases}\nx\\quad\\textrm{pour}\\quad x\\in[0,1]\\\\\n2-x\\quad\\textrm{pour}\\quad x\\in[1,2]\\\\\n0\\quad\\textrm{sinon}\n\\end{cases}\nPlot the density, cumulative dendity and inverse cumulative function\n\nusing Distributions\nusing Plots\nusing LaTeXStrings\na = 0; b = 2;\ndist = TriangularDist(a,b,1)  # min = a; max = b; mode = 1\nprintln(\"type de dist = \",typeof(dist))\nprintln(\"params(dist) = \", params(dist))\n\n\n# Density function\np1 = plot(x_grid, x-&gt;pdf(dist,x), color = :blue, linewidth=2, xlabel=(L\"x\"), ylabel=(L\"f(x)\")) \n# Cumulative density function\np2 = plot(a-1:0.01:b+1, x-&gt;cdf(dist,x), linewidth=2, xlabel=(L\"x\"), ylabel=(L\"F(x)\"))  \n# Inverse cumulative density function\np3 = plot(0:0.01:1, x-&gt;quantile(dist,x), xlims=(0,1), ylims=(0,2), color = :green, linewidth=2, xlabel=(L\"u\"), ylabel=(L\"F^{-1}(u)\"))\nplot(p1,p2,p3, layout=(1,3),legend = false,size = (1200,300), margin = 0.6Plots.cm)\n\ntype de dist = TriangularDist{Float64}\nparams(dist) = (0.0, 2.0, 1.0)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHistogram\nGenerate a sample of 100 datas from the triangular distribution and plot on the same graph the histogram of the simple and the PDF function\n\n# Sample of 100 datas\nt = rand(dist,100)\nhistogram(t) \nhistogram(t,normalize=true) # normalize=true pour ajouter la fonction de densité\nplot!(a-0.5:0.1:b+0.5, x-&gt;pdf.(dist,x), linewidth=2, xlabel=(L\"x\"), ylabel=(L\"f(x)\"))   \n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\nWhat is the problem ?\n\nUse the normalize=true parameter in the histogram function for solving the problem\nExecute for a sample of N = 10000 datas\n\n\n\n\nExample of discret distribution : the binomial distribution\n\nn, p, N = 10, 0.2, 10^3\nbDist = Binomial(n,p)\nxgrid = 0:n\nplot(xgrid,pdf.(bDist,xgrid), color=:orange, seriestype = :scatter)\nplot!(xgrid,pdf.(bDist,xgrid), line = :stem, linewidth=2, color=:orange)"
  },
  {
    "objectID": "M2/resources/TP1-distributions.html#central-limit-theorem",
    "href": "M2/resources/TP1-distributions.html#central-limit-theorem",
    "title": "Introduction to julia for statistic",
    "section": "Central limit theorem",
    "text": "Central limit theorem\nWe are going to illustrate the central limit theorem :\nSuppose X_1,X_2,\\ldots is a sequence of Independent and identically distributed random variables with E(X_i)=\\mu and Var(X_i)=\\sigma^2 &lt; +\\infty. Then, as n approaches infinity, the random variables \\sqrt{n}(\\bar{X}_n - \\mu) converge in distribution to a normal distribution \\mathcal{N}(0,\\sigma^2)\n\nExercise\n\nChoose a distribution law dist, compute its mean \\mu and its variance \\sigma^2 and N the number of sanple\nFor n in (1,2,5,20)\n\nGenerate N=10000 samples of lenght n from the dist distribution\nCompute the means of the N samples and the N values \\sqrt{n}(\\bar{X}_n - \\mu)\nPlot the histogram of these N values and the normal distribution \\mathcal{N}(0,\\sigma^2)\n\n\n\nX = rand(dist,(2,3))\nprintln(X)\n(mean(X,dims=1))\n\n[1.6238502970841644 1.6152917617470268 0.6276948740727439; 1.9019416578611044 0.9174508739539737 1.258732748878705]\n\n\n1×3 Matrix{Float64}:\n 1.7629  1.26637  0.943214\n\n\n\ndist = Uniform(0,12)\nμ = mean(dist)\nσ = std(dist)\nnormal_dist = Normal(0,σ)\nprintln(L\"\\mu = \", μ)\nprintln(L\"\\sigma = \", σ)\nN = 10000\nn_mean = (1,2,5,20)\n\np = []\nfor n in n_mean \n    X = rand(dist,(n,N))\n    Xbar_n = vec(sqrt(n)*(mean(X,dims=1) .- μ)) # to have a vector and not a matrix of size (1,3)\n    \n    p1 = histogram(Xbar_n,normalize=true) # normalize=true pour ajouter la fonction de densité\n    plot!(p1, -3*σ:0.1:3*σ, x-&gt;pdf.(normal_dist,x), linewidth=2, xlabel=(L\"x\"), ylabel=(L\"f(x)\"))   \n    push!(p,p1)\nend\nprintln(p)\nplot(p[1],p[2],p[3],p[4],legend = false)\n\n$\\mu = $6.0\n$\\sigma = $3.4641016151377544\nAny[Plot{Plots.GRBackend() n=2}, Plot{Plots.GRBackend() n=2}, Plot{Plots.GRBackend() n=2}, Plot{Plots.GRBackend() n=2}]"
  },
  {
    "objectID": "M2/exercices-linear_regression.html",
    "href": "M2/exercices-linear_regression.html",
    "title": "Lecture 2",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Master 2",
      "Tutorials",
      "Linear regression"
    ]
  },
  {
    "objectID": "M1/getting_started-julia_ecosystem.html",
    "href": "M1/getting_started-julia_ecosystem.html",
    "title": "Julia ecosystem",
    "section": "",
    "text": "In this page, we present the Julia ecosystem: the Julia programming language, JuliaHub, JuliaSim, JuliaCon, etc.",
    "crumbs": [
      "Master 1",
      "Getting started",
      "Julia ecosystem"
    ]
  },
  {
    "objectID": "M1/getting_started-julia_ecosystem.html#a-brief-presentation",
    "href": "M1/getting_started-julia_ecosystem.html#a-brief-presentation",
    "title": "Julia ecosystem",
    "section": "A brief presentation",
    "text": "A brief presentation\n\nThe work on Julia began in 2009 when Jeff Bezanson, Stefan Karpinski, Alan Edelman and Viral B. Shah set out to create a free language that was both high-level and fast. The first public appearance is in 2012. Julia’s syntax is now considered stable, since version 1.0 in 2018. Julia is a high-level, general-purpose dynamic programming language, still designed to be fast and productive, for e.g. data science, artificial intelligence, machine learning, modeling and simulation, most commonly used for numerical analysis and computational science. See the Julia Wikipedia page for more details.\nOn the Julia official webpage one can find the following summary of Julia possibilities.\n\nVisualizationGeneralData ScienceMachine LearningScientific DomainsParallel Computing\n\n\n\n\n\nData Visualization and Plotting\nData visualization has a complicated history. Plotting software makes trade-offs between features and simplicity, speed and beauty, and a static and dynamic interface. Some packages make a display and never change it, while others make updates in real-time.\nPlots.jl is a visualization interface and toolset. It provides a common API across various backends, like GR.jl, PyPlot.jl, and PlotlyJS.jl. Makie.jl is a sophisticated package for complex graphics and animations. Users who are used to “grammar of graphics” plotting APIs should take a look at Gadfly.jl. VegaLite.jl provides the Vega-Lite grammar of interactive graphics interface as a Julia package. For those who do not wish to leave the comfort of the terminal, there is also UnicodePlots.jl.\n\n\n\n\n\nBuild, Deploy or Embed Your Code\nJulia makes it possible to build complete applications. Write web UIs with Dash.jl and Genie.jl or native UIs with Gtk4.jl. Pull data from a variety of databases. Build shared libraries and executables with PackageCompiler. Deploy on a webserver with HTTP.jl or embedded devices. Powerful shell integration make it easy to managing other processes.\nJulia has foreign function interfaces for C, Fortran, C++, Python, R, Java, Mathematica, Matlab, and many other languages. Julia can also be embedded in other programs through its embedding API. Julia’s PackageCompiler makes it possible to build binaries from Julia programs that can be integrated into larger projects. Python programs can call Julia using juliacall. R programs can do the same with R’s JuliaCall, which is demonstrated by calling MixedModels.jl from R. Mathematica supports calling Julia through its External Evaluation System.\n\n\n\n\n\nInteract with your Data\nThe Julia data ecosystem provides DataFrames.jl to work with datasets, and perform common data manipulations. CSV.jl is a fast multi-threaded package to read CSV files and integration with the Arrow ecosystem is in the works with Arrow.jl. Online computations on streaming data can be performed with OnlineStats.jl. The Queryverse provides query, file IO and visualization functionality. In addition to working with tabular data, the JuliaGraphs packages make it easy to work with combinatorial data.\nJulia can work with almost all databases using JDBC.jl and ODBC.jl drivers. In addition, it also integrates with the Spark ecosystem through Spark.jl.\n\n\n\n\n\nScalable Machine Learning\nThe MLJ.jl package provides a unified interface to common machine learning algorithms, which include generalized linear models, decision trees, and clustering. Flux.jl and Lux.jl are powerful packages for Deep Learning. Packages such as Metalhead.jl, ObjectDetector.jl, and TextAnalysis.jl provide ready to use pre-trained models for common tasks. AlphaZero.jl provides a high performance implementation of the reinforcement learning algorithms from AlphaZero. Turing.jl is a best in class package for probabilistic programming.\n\n\n\n\n\nRich Ecosystem for Scientific Computing\nJulia is designed from the ground up to be very good at numerical and scientific computing. This can be seen in the abundance of scientific tooling written in Julia, such as the state-of-the-art differential equations ecosystem (DifferentialEquations.jl), optimization tools (JuMP.jl and Optim.jl), iterative linear solvers (IterativeSolvers.jl), Fast Fourier transforms (AbstractFFTs.jl), and much more. General purpose simulation frameworks are available for Scientific Machine Learning, Quantum computing and much more.\nJulia also offers a number of domain-specific ecosystems, such as in biology (BioJulia), operations research (JuMP Dev), image processing (JuliaImages), quantum physics (QuantumBFS), nonlinear dynamics (JuliaDynamics), quantitative economics (QuantEcon), astronomy (JuliaAstro) and ecology (EcoJulia). With a set of highly enthusiastic developers and maintainers, the scientific ecosystem in Julia continues to grow rapidly.\n\n\n\n\n\nParallel and Heterogeneous Computing\nJulia is designed for parallelism, and provides built-in primitives for parallel computing at every level: instruction level parallelism, multi-threading, GPU computing, and distributed computing. The Celeste.jl project achieved 1.5 PetaFLOP/s on the Cori supercomputer at NERSC using 650,000 cores.\nThe Julia compiler can also generate native code for GPUs. Packages such as DistributedArrays.jl and Dagger.jl provide higher levels of abstraction for parallelism. Distributed Linear Algebra is provided by packages like Elemental.jl and TSVD.jl. MPI style parallelism is also available through MPI.jl.",
    "crumbs": [
      "Master 1",
      "Getting started",
      "Julia ecosystem"
    ]
  },
  {
    "objectID": "M1/getting_started-julia_ecosystem.html#juliahub",
    "href": "M1/getting_started-julia_ecosystem.html#juliahub",
    "title": "Julia ecosystem",
    "section": "JuliaHub",
    "text": "JuliaHub\nJulia is a modern language and comes with cloud computing and more, see the JuliaHub webpage and the JuliaHub help page.\n\nThe Modern Platform for Technical Computing. A single place for modeling, simulation, and user built applications with the Julia language. Designed with access to CPUs and GPUs for multi-threading, parallel and distributed computing, JuliaHub’s supercomputing infrastructure allows teams to model breakthrough science and technology.\n\nWith JuliaHub, comes JuliaSim.\n\nJuliaSim is the next-generation, cloud-based platform for model-based design. Using modern scientific machine learning (SciML) techniques and equation-based digital twin modeling and simulation, JuliaSim accelerates simulation times, significantly reducing workflow runtime from months to hours. JuliaSim encompasses block diagrams, acausal modeling, state transition diagram and a differentiable programming language all within a single environment.",
    "crumbs": [
      "Master 1",
      "Getting started",
      "Julia ecosystem"
    ]
  },
  {
    "objectID": "M1/getting_started-julia_ecosystem.html#juliacon",
    "href": "M1/getting_started-julia_ecosystem.html#juliacon",
    "title": "Julia ecosystem",
    "section": "JuliaCon",
    "text": "JuliaCon\nThe Julia programming language has its own conference, see the JuliaCon 2024.\n\nWelcome to JuliaCon 2024, the premier annual conference for the Julia programming language community! This exciting event brings together developers, researchers, and enthusiasts from around the globe to celebrate and explore the power and versatility of Julia.",
    "crumbs": [
      "Master 1",
      "Getting started",
      "Julia ecosystem"
    ]
  },
  {
    "objectID": "M1/getting_started-julia_ecosystem.html#editors-and-ides",
    "href": "M1/getting_started-julia_ecosystem.html#editors-and-ides",
    "title": "Julia ecosystem",
    "section": "Editors and Ides",
    "text": "Editors and Ides\nMost computer programs are just plain text files with a specific extension (in our case .jl). So in theory, any text editor suffices to write and modify Julia code. In practice, an Integrated Development Environment (or IDE) makes the experience much more pleasant, thanks to code-related utilities and language-specific plugins.\nThe best IDE for Julia is Visual Studio Code, or VS Code, developed by Microsoft. Indeed, the Julia VS Code extension is the most feature-rich of all Julia IDE plugins. You can download it from the VS Code Marketplace and read its documentation.\n\n\n\n\n\n\nTip\n\n\n\nTo use Julia in Visual Studio Code, please follow these steps, where you can find how to install Julia, VS Code and the necessary extensions.\n\n\n\nVS CodeJupyterPluto.jlVimEmacs\n\n\n  \nJulia for Visual Studio Code is a powerful, free IDE for the Julia language. Visual Studio Code is a powerful and customizable editor. With a completely live environment, Julia for VS Code aims to take the frustration and guesswork out of programming and put the fun back in. We build on Julia’s unique combination of ease-of-use and performance. Beginners and experts can build better software more quickly, and get to a result faster. Julia is an officially supported language on the VS Code documentation.\n\n\n  \nYou can write Jupyter notebooks and use the Jupyter interactive environment. To do so you need to install the IJulia.jl package. IJulia is a Julia-language backend combined with the Jupyter interactive environment (also used by IPython). This combination allows you to interact with the Julia language using Jupyter/IPython’s powerful graphical notebook, which combines code, formatted text, math, and multimedia in a single document. IJulia is a Jupyter language kernel and works with a variety of notebook user interfaces. In addition to the classic Jupyter Notebook, IJulia also works with JupyterLab, a Jupyter-based integrated development environment for notebooks and code. The nteract notebook desktop supports IJulia with detailed instructions for its installation with nteract. Please visit this webpage to get a tutorial on how to use Julia in Jupyter notebook.\n\n\n   Simple reactive notebooks\n\n\n   Vim plugin\n\n\n   Emacs plugin",
    "crumbs": [
      "Master 1",
      "Getting started",
      "Julia ecosystem"
    ]
  },
  {
    "objectID": "M1/getting_started-julia_ecosystem.html#documentation",
    "href": "M1/getting_started-julia_ecosystem.html#documentation",
    "title": "Julia ecosystem",
    "section": "Documentation",
    "text": "Documentation\nIn the Julia documentation, you may find:\n\nsome important Links that can be useful to learn and use the Julia programming language,\na comparison to other languages,\na list of advantages of using Julia,\n\nbut also:\n\nmanuals;\nbase documentation;\nstandard librairies documentation;\ndeveloper documentation.\n\n\n\n\n\n\n\nNote\n\n\n\nIn Julia, it is quite common to have the documentation in the form of manuals or tutorials. See Getting Started with Differential Equations in Julia for instance.\n\n\nExample. Search how to compute the norm of a vector.\n\nThe norm function comes from the standard librairy LinearAlgebra.\n\nHowever, we are not directly at the right place, so I recommend to use the Search docs field.\n\nAnd now, we have the documentation.\n\n\n\n\n\n\n\nNote\n\n\n\nIt is also possible to use the help mode:\nusing LinearAlgebra\n?norm",
    "crumbs": [
      "Master 1",
      "Getting started",
      "Julia ecosystem"
    ]
  },
  {
    "objectID": "M1/getting_started-julia_ecosystem.html#packages",
    "href": "M1/getting_started-julia_ecosystem.html#packages",
    "title": "Julia ecosystem",
    "section": "Packages",
    "text": "Packages\nWhen you install Julia, you download the standard library which contains a list of standard packages to manipulate files, do some linear algebra and more. To use a package, you need first to import it. For instance, to compute the norm of a vector, you need to import LinearAlgebra.jl.\n\nusing LinearAlgebra\nv = [3, 4]\nnorm(v)\n\n5.0\n\n\nFrom the community you will find thousands of other packages. See the list of packages here. Usually, a Julia package is hosted on GitHub and comes with an online documentation. For instance, the package Plots.jl is hosted here and you can find the documention here.\nA very interesting point in Julia is that the language comes with its own package manager. To install and use Plots.jl, you will simply have to write the following lines of code.\n\nusing Pkg\nPkg.add(\"Plots\")\nusing Plots\nplot(x-&gt;x^2, -1, 1) # we plot the function f(x)=x^2 in [-1, 1]\n\n   Resolving package versions...\n  No Changes to `~/Courses/julia/course-tse-julia/M1/Project.toml`\n  No Changes to `~/Courses/julia/course-tse-julia/M1/Manifest.toml`",
    "crumbs": [
      "Master 1",
      "Getting started",
      "Julia ecosystem"
    ]
  },
  {
    "objectID": "M1/getting_started-foretaste_of_julia_code.html",
    "href": "M1/getting_started-foretaste_of_julia_code.html",
    "title": "Foretaste of Julia code",
    "section": "",
    "text": "x = 1\n2x\n\n2\n\n\n\nx = sqrt(2)\n\n1.4142135623730951\n\n\n\n# unicode is great\nx = √(2)\n\n1.4142135623730951\n\n\n\n😄 = sqrt(2)\n2😄\n\n2.8284271247461903\n\n\nSome functions.\n\n# this is a function\nfunction f(x)\n  return 2x+1\nend\n\nf (generic function with 1 method)\n\n\n\n# this also\nf(x) = 2x+1\n\nf (generic function with 1 method)\n\n\n\n# this also: but f is not available anymore\ng = x -&gt; 2x^2\n\n#20 (generic function with 1 method)\n\n\n\n# be careful of the priorities\ng(1)\n\n2\n\n\n\nHello(name) = \"Hello \" * name * \"!\"\nHello(\"Alban\")\n\n\"Hello Alban!\"\n\n\n\nx = [1 3 12]\nprintln(\"x[2] = \", x[2])\nx[2] = 5\nprintln(\"x[2] = \", x[2])\n\nx[2] = 3\nx[2] = 5\n\n\nThe following function has side effects that can be dangerous.\n\nfunction f(x, y)\n    x[1] = 42      # mutates x\n    y = 7 + sum(x) # new binding for y, no mutation\n    return y\nend\n\na = [4, 5, 6]\nb = 3\n\nprintln(\"f($a, $b) = \", f(a, b))\nprintln(\"a = \", a, \" # a[1] is changed to 42 by f\")\nprintln(\"b = \", b, \" # not changed\")\n\nf([4, 5, 6], 3) = 60\na = [42, 5, 6] # a[1] is changed to 42 by f\nb = 3 # not changed\n\n\nWhen a function has side effects, please use the ! convention. See Argument Passing Behavior.\n\nfunction put_at_second_place!(x, value)\n  x[2] = value\n  return nothing\nend\n\nx = [1 3 12]\nprintln(\"x[2] = \", x[2])\n\nput_at_second_place!(x, 5)\nprintln(\"x[2] = \", x[2])\n\nput_at_second_place!(x[1:3], 15) # be careful if you give a slice\nprintln(\"x[2] = \", x[2])\n\nx[2] = 3\nx[2] = 5\nx[2] = 5\n\n\nA function may have several methods.\n\nΣ(x::Float64, y::Float64) = 2x + y\n\nΣ (generic function with 1 method)\n\n\n\nΣ(2.0, 3.0)\n\n7.0\n\n\n\nΣ(2, 3.0)\n\n\nMethodError: no method matching Σ(::Int64, ::Float64)\nThe function `Σ` exists, but no method is defined for this combination of argument types.\n\nClosest candidates are:\n  Σ(::Float64, ::Float64)\n   @ Main In[15]:1\n\n\nStacktrace:\n [1] top-level scope\n   @ In[17]:1\n\n\n\n\nh(x::Number,  y::Number ) = 2x - y\nh(x::Int,     y::Int    ) = 2x * y\nh(x::Float64, y::Float64) = 2x + y\n\nh (generic function with 3 methods)\n\n\n\nprintln(\"h(2,   3.0) = \", h(2, 3.0))\nprintln(\"h(2,   3  ) = \", h(2, 3))\nprintln(\"h(2.0, 3.0) = \", h(2.0, 3.0))\n\nh(2,   3.0) = 1.0\nh(2,   3  ) = 12\nh(2.0, 3.0) = 7.0\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Master 1",
      "Getting started",
      "Foretaste of Julia code"
    ]
  },
  {
    "objectID": "M1/getting_started-exercice_simple_linear_regression.html",
    "href": "M1/getting_started-exercice_simple_linear_regression.html",
    "title": "Exercise: Simple Linear Regression",
    "section": "",
    "text": "We propose a first exercise about simple linear regression. The data are excerpted from this example and saved into data.csv. We propose an ordinary least squares formulation which is a type of linear least squares method for choosing the unknown parameters in a linear regression model by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being observed) in the input dataset and the output of the (linear) function of the independent variable.\n\nGiven a set of m data points y_{1}, y_{2}, \\dots, y_{m}, consisting of experimentally measured values taken at m values x_{1}, x_{2}, \\dots, x_{m} of an independent variable (x_i may be scalar or vector quantities), and given a model function y=f(x,\\beta), with \\beta =(\\beta_{1},\\beta_{2},\\dots ,\\beta_{n}), it is desired to find the parameters \\beta_j such that the model function “best” fits the data. In linear least squares, linearity is meant to be with respect to parameters \\beta_j, so \n  f(x, \\beta) = \\sum_{j=1}^n \\beta_j\\, \\varphi_j(x).\n In general, the functions \\varphi_j may be nonlinear. However, we consider linear regression, that is \n  f(x, \\beta) = \\beta_1 + \\beta_2 x.\n Ideally, the model function fits the data exactly, so \n  y_i = f(x_i, \\beta)\n for all i=1, 2, \\dots, m. This is usually not possible in practice, as there are more data points than there are parameters to be determined. The approach chosen then is to find the minimal possible value of the sum of squares of the residuals \n  r_i(\\beta) = y_i - f(x_i, \\beta), \\quad i=1, 2, \\dots, m\n so to minimize the function \n  S(\\beta) = \\sum_{i=1}^m r_i^2(\\beta).\n In the linear least squares case, the residuals are of the form \n  r(\\beta) = y - X\\, \\beta\n with y = (y_i)_{1\\le i\\le m} \\in \\mathbb{R}^m and X = (X_{ij})_{1\\le i\\le m, 1\\le j\\le n} \\in \\mathrm{M}_{mn}(\\mathbb{R}), where X_{ij} = \\varphi_j(x_i). Since we consider linear regression, the i-th row of the matrix X is given by \n  X_{i[:]} = [1 \\quad x_i].\n The objective function may be written \n  S(\\beta) = {\\Vert y - X\\, \\beta \\Vert}^2\n where the norm is the usual 2-norm. The solution to the linear least squares problem \n  \\underset{\\beta \\in \\mathbb{R}^n}{\\mathrm{minimize}}\\, {\\Vert y - X\\, \\beta \\Vert}^2\n is computed by solving the normal equation \n  X^\\top X\\, \\beta = X^\\top y,\n where X^\\top denotes the transpose of X.",
    "crumbs": [
      "Master 1",
      "Getting started",
      "Exercice: Simple Linear Regression"
    ]
  },
  {
    "objectID": "M1/getting_started-exercice_simple_linear_regression.html#least-squares-regression-line",
    "href": "M1/getting_started-exercice_simple_linear_regression.html#least-squares-regression-line",
    "title": "Exercise: Simple Linear Regression",
    "section": "",
    "text": "We propose a first exercise about simple linear regression. The data are excerpted from this example and saved into data.csv. We propose an ordinary least squares formulation which is a type of linear least squares method for choosing the unknown parameters in a linear regression model by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being observed) in the input dataset and the output of the (linear) function of the independent variable.\n\nGiven a set of m data points y_{1}, y_{2}, \\dots, y_{m}, consisting of experimentally measured values taken at m values x_{1}, x_{2}, \\dots, x_{m} of an independent variable (x_i may be scalar or vector quantities), and given a model function y=f(x,\\beta), with \\beta =(\\beta_{1},\\beta_{2},\\dots ,\\beta_{n}), it is desired to find the parameters \\beta_j such that the model function “best” fits the data. In linear least squares, linearity is meant to be with respect to parameters \\beta_j, so \n  f(x, \\beta) = \\sum_{j=1}^n \\beta_j\\, \\varphi_j(x).\n In general, the functions \\varphi_j may be nonlinear. However, we consider linear regression, that is \n  f(x, \\beta) = \\beta_1 + \\beta_2 x.\n Ideally, the model function fits the data exactly, so \n  y_i = f(x_i, \\beta)\n for all i=1, 2, \\dots, m. This is usually not possible in practice, as there are more data points than there are parameters to be determined. The approach chosen then is to find the minimal possible value of the sum of squares of the residuals \n  r_i(\\beta) = y_i - f(x_i, \\beta), \\quad i=1, 2, \\dots, m\n so to minimize the function \n  S(\\beta) = \\sum_{i=1}^m r_i^2(\\beta).\n In the linear least squares case, the residuals are of the form \n  r(\\beta) = y - X\\, \\beta\n with y = (y_i)_{1\\le i\\le m} \\in \\mathbb{R}^m and X = (X_{ij})_{1\\le i\\le m, 1\\le j\\le n} \\in \\mathrm{M}_{mn}(\\mathbb{R}), where X_{ij} = \\varphi_j(x_i). Since we consider linear regression, the i-th row of the matrix X is given by \n  X_{i[:]} = [1 \\quad x_i].\n The objective function may be written \n  S(\\beta) = {\\Vert y - X\\, \\beta \\Vert}^2\n where the norm is the usual 2-norm. The solution to the linear least squares problem \n  \\underset{\\beta \\in \\mathbb{R}^n}{\\mathrm{minimize}}\\, {\\Vert y - X\\, \\beta \\Vert}^2\n is computed by solving the normal equation \n  X^\\top X\\, \\beta = X^\\top y,\n where X^\\top denotes the transpose of X.",
    "crumbs": [
      "Master 1",
      "Getting started",
      "Exercice: Simple Linear Regression"
    ]
  },
  {
    "objectID": "M1/getting_started-exercice_simple_linear_regression.html#questions",
    "href": "M1/getting_started-exercice_simple_linear_regression.html#questions",
    "title": "Exercise: Simple Linear Regression",
    "section": "Questions",
    "text": "Questions\nTo answer the questions you need to import the following packages.\n\nusing DataFrames\nusing CSV\nusing Plots\n\nYou also need to download the csv file. Click on the following image.\n  \n\nUsing the packages DataFrames.jl and CSV.jl, load the dataset from data/introduction/data.csv and save the result into a variable named dataset.\n\n\n\nShow the answer\npath = \"data/introduction/data.csv\" # update depending on the location of your file\ndataset = DataFrame(CSV.File(path))\n\n\n5×2 DataFrame\n\n\n\nRow\nTime\nMass\n\n\n\nInt64\nInt64\n\n\n\n\n1\n5\n40\n\n\n2\n7\n120\n\n\n3\n12\n180\n\n\n4\n16\n210\n\n\n5\n20\n240\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDo not hesitate to visit the documentation of CSV.jl and DataFrames.jl.\n\n\n\nUsing the package Plot.jl, plot the data.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse names(dataset) to get the list of data names. If Time is a name you can access to the associated data by dataset.Time.\n\n\n\n\n\nShow the answer\nplt = plot(\n  dataset.Time, \n  dataset.Mass,\n  seriestype=:scatter, \n  legend=false, \n  xlabel=\"Time\", \n  ylabel=\"Mass\"\n)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreate the matrix X, the vector \\beta and solve the normal equation with the operator Base.\\.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse ones(m) to generate a vector of 1 of length m.\n\n\n\n\n\nShow the answer\nm = length(dataset.Time)\nX = [ones(m) dataset.Time]\ny = dataset.Mass\nβ = X\\y\n\n\n2-element Vector{Float64}:\n 11.506493506493449\n 12.207792207792208\n\n\n\nPlot the linear model on the same plot as the data. Use the plot! function. See the basic concepts for plotting.\n\n\n\nShow the answer\nx = [5, 20]\ny = β[1] .+ β[2]*x\nplot!(plt, x, y)",
    "crumbs": [
      "Master 1",
      "Getting started",
      "Exercice: Simple Linear Regression"
    ]
  },
  {
    "objectID": "M1/index.html",
    "href": "M1/index.html",
    "title": "Julia Master 1 course",
    "section": "",
    "text": "This course is adressed to the students of the Master 1 “Econométrie, Statistiques” of Toulouse School of Economics. It is part of the topic Software for Data science. The Julia course is composed of 5 slots of 3 hours. In this course, we introduce the Julia ecosystem, we present the Julia programming language and some relevant packages.\n\n\n\n\n\nGetting started\n\nIntroduction to Julia ecosystem\n\n\n\nJulia ecosystem\nExecute some code\nForetaste of Julia code\nExample: Simple Linear Regression\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Master 1"
    ]
  },
  {
    "objectID": "M1/getting_started-execute_some_code.html",
    "href": "M1/getting_started-execute_some_code.html",
    "title": "Executing Julia code",
    "section": "",
    "text": "This part is partially inspired from the post Writing your code of the website Modern Julia Workflows, where you can find tips to make the coding experience more pleasant and efficient.",
    "crumbs": [
      "Master 1",
      "Getting started",
      "Execute some code"
    ]
  },
  {
    "objectID": "M1/getting_started-execute_some_code.html#getting-help",
    "href": "M1/getting_started-execute_some_code.html#getting-help",
    "title": "Executing Julia code",
    "section": "Getting help",
    "text": "Getting help\nBefore you write any line of code, it’s good to know where to find help. The official help page is a good place to start. In particular, the Julia community is always happy to guide beginners.\nAs a rule of thumb, the Discourse forum is where you should ask your questions to make the answers discoverable for future users. If you just want to chat with someone, you have a choice between the open source Zulip and the closed source Slack. Some of the vocabulary used by community members may appear unfamiliar, but don’t worry: StartHere.jl gives you a good overview.",
    "crumbs": [
      "Master 1",
      "Getting started",
      "Execute some code"
    ]
  },
  {
    "objectID": "M1/getting_started-execute_some_code.html#installation",
    "href": "M1/getting_started-execute_some_code.html#installation",
    "title": "Executing Julia code",
    "section": "Installation",
    "text": "Installation\nJulia can be easily downloaded and installed. See the download page where we can find the following.\n\nMac / LinuxWindows\n\n\nInstall the latest Julia version by running this in your terminal:\ncurl -fsSL https://install.julialang.org | sh\n\n\nInstall the latest Julia version from the Microsoft Store by running this in the command prompt:\nwinget install julia -s msstore\n\n\n\nThere are different ways to write and execute some Julia code:\n\nwithin a .ipynb file, that is a Jupyter notebook;\ndirectly into the Julia REPL;\nfrom a script file .jl, either in the REPL or in the terminal;\n…",
    "crumbs": [
      "Master 1",
      "Getting started",
      "Execute some code"
    ]
  },
  {
    "objectID": "M1/getting_started-execute_some_code.html#jupyter-notebooks",
    "href": "M1/getting_started-execute_some_code.html#jupyter-notebooks",
    "title": "Executing Julia code",
    "section": "Jupyter notebooks",
    "text": "Jupyter notebooks\nYou can write some Julia code inside a Jupyter notebook and execute the code. To use it with Julia, you will need to install the IJulia.jl backend. Then, if you have also installed Jupyter with pip install jupyterlab, you can run this command to launch the server:\njupyter lab\nIf you only have IJulia.jl on your system, you can run this:\nusing IJulia\nIJulia.notebook()\n\n\n\n\n\n\nTip\n\n\n\nJupyter notebooks can be opened, modified and run directly from VS Code. Thanks to the Julia extension, you don’t even need to install IJulia.jl or Jupyter first.\n\n\nTry to execute the code of example.ipynb. You can dowload it clicking on the following image.",
    "crumbs": [
      "Master 1",
      "Getting started",
      "Execute some code"
    ]
  },
  {
    "objectID": "M1/getting_started-execute_some_code.html#repl",
    "href": "M1/getting_started-execute_some_code.html#repl",
    "title": "Executing Julia code",
    "section": "REPL",
    "text": "REPL\nJulia comes with a full-featured interactive command-line REPL (read-eval-print loop) built into the julia executable. In addition to allowing quick and easy evaluation of Julia statements, it has a searchable history, tab-completion, many helpful keybindings, and dedicated help and shell modes. The REPL can be started by simply calling julia with no arguments or double-clicking on the executable:\n\n\n$ julia\n\n               _\n   _       _ _(_)_     |  Documentation: https://docs.julialang.org\n  (_)     | (_) (_)    |\n   _ _   _| |_  __ _   |  Type \"?\" for help, \"]?\" for Pkg help.\n  | | | | | | |/ _` |  |\n  | | |_| | | | (_| |  |  Version 1.11.1 (2024-10-16)\n _/ |\\__'_|_|_|\\__'_|  |  Official https://julialang.org/ release\n|__/                   |\n\n\njulia&gt;\n\n\nAfter Julia is launched you can start computing.\n\n1+1\n\n2\n\n\nOr print some text.\n\nprint(\"Hello!\")\n\nHello!\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe Julia REPL has different prompt modes that can be very useful to install / remove packages, run shell commands, search for help, etc. The different modes are:\n\nThe Julian mode\nHelp mode\nPackage mode\nShell mode\n\nVisit the command-line REPL page for more details.\n\n\n\nHelp modePackage modeShell mode\n\n\nBy pressing ? you can obtain information and metadata about Julia objects (functions, types, etc.) or unicode symbols. The query fetches the docstring of the object, which explains how to use it.\nhelp?&gt; println\nIf you don’t know the exact name you are looking for, type a word surrounded by quotes to see in which docstrings it pops up. To come back to Julia mode, hit backspace.\n\n\nBy pressing ] you access Pkg.jl, Julia’s integrated package manager. Please visit the documentation for details. Pkg.jl allows you to:\n\n]activate different local, shared or temporary environments;\n]instantiate them by downloading the necessary packages;\n]add, ]update (or ]up) and ]remove (or ]rm) packages;\nget the ]status (or ]st) of your current environment.\n\nAs an illustration, we download the package Plots.jl inside our current environment:\npkg&gt; add Plots\nNote that you can do the same in Julia mode:\njulia&gt; using Pkg\njulia&gt; Pkg.rm(\"Plots\")\nThe package mode itself also has a help mode, accessed with ?. To come back to Julia mode, hit backspace.\n\n\nBy pressing ; you enter a terminal, where you can execute any command you want. Here’s an example for Unix systems:\nshell&gt; pwd\nTo come back to Julia mode, hit backspace.",
    "crumbs": [
      "Master 1",
      "Getting started",
      "Execute some code"
    ]
  },
  {
    "objectID": "M1/getting_started-execute_some_code.html#julia-scripts",
    "href": "M1/getting_started-execute_some_code.html#julia-scripts",
    "title": "Executing Julia code",
    "section": "Julia scripts",
    "text": "Julia scripts\nYou can also write some code in a .jl script.\n\n\n$ cat example.jl\n\nprintln(\"Hello!\")\n\n\nand execute the code either in the REPL:\n\ninclude(\"example.jl\")\n\nHello!\n\n\nor in a terminal with the command $ julia example.jl.\nTry to execute the code of example.jl. You can dowload it clicking on the following image.",
    "crumbs": [
      "Master 1",
      "Getting started",
      "Execute some code"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\n\n Back to top"
  },
  {
    "objectID": "M2/resources/TP2-linear-regression.html",
    "href": "M2/resources/TP2-linear-regression.html",
    "title": "TP2 : Linear regression",
    "section": "",
    "text": "We suppose that for i=1,\\ldots,n Y_i=\\beta_0+\\beta_1 x+\\varepsilon_i\nwith - E(\\varepsilon_i)=0 - The (\\varepsilon_i)_{i=1,\\dots,n} are i.i.d. - The distributions of (\\varepsilon_i)_{i=1,\\dots,n} are \\mathcal{N}(0,\\sigma^2)\n\n\n\nWe use the mean square estimation of the parameters \\beta_0 and \\beta_1:\n\\begin{array}{ll}\nMin\\quad\nf(\\beta)=\\frac{1}{2}\\sum_{i=1}^{n}r_{i}^{2}(\\beta)& =\\frac{1}{2}\\|r(\\beta)\\|^2\\\\\n\\beta \\in \\mathbf{R}^{2}&\n\\end{array}\nwhere r is the residual function r:\\mathbf{R}^2\\rightarrow \\mathbf{R}^n r_i(\\beta) =  y_i - \\beta_{0}+\\beta_{1}x_{i}\nSo we can write r(\\beta) = y - X\\beta, where\ny = \\begin{pmatrix} y_1\\\\ \\vdots \\\\ y_n\\end{pmatrix}\\;\\,\\texttt{and}\\;\\;\nX = \\begin{pmatrix} 1 & x_1\\\\ \\vdots & \\vdots \\\\ 1 & x_n\\end{pmatrix}\n\nusing Test \nusing DataFrames\nusing Plots\nusing LaTeXStrings\nusing Statistics\nusing DataFrames\nusing GLM\n\ny=[1.8, 2.3, 2, 2.4, 2, 2.5, 2.6, 2.6, 2.9, 2.3, 2.4, 2.1, 2.5, 2.7, 2.7, 3, 3.1, 2.3, 2.5, 3, 3.3, 2.7, 3.1, 2.9, 3.4, 2.4, 3.4, 3.7, 2.8, 3.3, 3.5, 3.3, 2.6]\nx=[25, 25, 25, 25, 25, 25, 25, 35, 35, 35, 35, 35, 35, 35, 45, 45, 45, 45, 45, 45, 45, 45, 55, 55, 55, 55, 55, 65, 65, 65, 65, 65, 65]\nn = length(x)\n\np = plot(x,y, seriestype = :scatter, xlabel = (L\"x\"), ylabel = (L\"y\"))\na , b = .5 , 0.04\nplot!(p,x,a .+ b*x)\n\nr(a,b) = y - (a .+ b*x)\n\nr(a,b)\n\ni = 11\nplot!(p, [x[i],x[i]], [y[i],a+b*x[i]],linewidth=3)\nannotate!(p,x[i],mean([y[i],a+b*x[i]]),text(L\"r_{11}(a,b)\", :left, :green))\nplot(p,legend=false)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor solving the optimization problem we have to sole the linear system X^TX\\beta = X^Ty\nUse the  operator for solving linear equation\n\n# \n# Use of the GLM Package\ndatas = DataFrame(age=x, chol = y)\nlm1 = lm(@formula(chol ~ age), datas)\nprintln(coef(lm1))\nprintln(lm1)\n\n# ... To complete\nX = [ones(n) x]\n\nβhat₀, βhat₁ = (X'*X)\\(X'*y)\nβhat = [βhat₀, βhat₁]\nprintln(\"Normal equation : βhat₀, βhat₁ = \", βhat₀, \", \", βhat₁)\nβhat₀, βhat₁ = X\\y\nprintln(\"\\\\ of julia : βhat₀, βhat₁ = \", βhat₀, \", \", βhat₁)\np = plot(x,y, seriestype = :scatter, xlabel = (L\"x\"), ylabel = (L\"y\"))\nplot!(p,x,βhat₀ .+ βhat₁*x)\n\nprintln(@test βhat ≈ coef(lm1))\n\n[1.639575381679391, 0.024909351145038124]\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n\nchol ~ 1 + age\n\nCoefficients:\n────────────────────────────────────────────────────────────────────────\n                 Coef.  Std. Error     t  Pr(&gt;|t|)  Lower 95%  Upper 95%\n────────────────────────────────────────────────────────────────────────\n(Intercept)  1.63958    0.189994    8.63    &lt;1e-09  1.25208    2.02707\nage          0.0249094  0.00413632  6.02    &lt;1e-05  0.0164733  0.0333454\n────────────────────────────────────────────────────────────────────────\nNormal equation : βhat₀, βhat₁ = 1.6395753816793879, 0.0249093511450382\n\\ of julia : βhat₀, βhat₁ = 1.6395753816793897, 0.02490935114503816\nTest Passed\n\n\n\n\n\n\nWe note r_i = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i), then the unbiased estimation of the variance \\sigma^2 is\n\n\\hat{\\sigma}^2\\frac{\\sum_ir_i^2}{n-2}\n\nAs \\hat{\\beta} = (X^TX)^{-1}X^Ty the statistic B=(X^TX)^{-1}X^TY = (X^TX)^{-1}X^T(X\\beta+\\varepsilon)=\\beta +(X^TX)^{-1}X^T\\varepsilon as the \\mathcal{N}(\\beta,\\sigma^2(X^TX)^{-1}) distribution.\n\nSi les postulats sont vérifiés alors les intervalles de confiances de \\beta _{0},\\beta _{1} et \\sigma ^{2} au seuil (1-\\alpha ) sont:\n\n\n\\sigma ^{2}\\in \\left[ \\frac{\\sum_{i=1}^{n}r_{i}^{2}}{\\chi _{1-\\alpha /2}};%\n\\frac{\\sum_{i=1}^{n}r_{i}^{2}}{\\chi _{\\alpha /2}}\\right]\n\n\nwhere \\chi _{1-\\alpha /2} and \\chi _{\\alpha /2} sont lues dans le table du Khi-2 à \\nu =n-2 ddl.\n\n\\begin{array}{ccc}\n\\beta _{1} & \\in  & [\\hat{\\beta}_{1}-t_{1-\\alpha /2}\\hat{\\sigma}%\n_{B_{1}}^{{}};\\hat{\\beta}_{1}+t_{1-\\alpha /2}\\hat{\\sigma}_{B_{1}}^{{}}] \\\\\n\\beta _{0} & \\in  & [\\hat{\\beta}_{0}-t_{1-\\alpha /2}\\hat{\\sigma}%\n_{B_{0}}^{{}};\\hat{\\beta}_{0}+t_{1-\\alpha /2}\\hat{\\sigma}_{B_{0}}^{{}}]\n\\end{array}\n\noù - ${B{1}}^{2} = $ - \\hat{\\sigma}_{B_{0}}^{2} = \\hat{\\sigma}^{2}(\\dfrac{1}{n}+\\dfrac{\\bar{x}^{2}}{SCE_{x}}) - t_{1-\\alpha /2} from the Student law with n-2 degrees of freedom\n\n\n\n\nCompute the n residuals in the vector res\nCompute the estimation of residual variance \\sigma^2 and of the standard error \\sigma\nCompute the estimation of the variance, covariance matrix \\Sigma.\nCompute the estimation of the standard error of \\hat{\\beta}_0 and \\hat{\\beta}_1.\nCompute the confidence intervalle of \\beta_0 and \\beta_1\n\n\n# Compute the residuals\nres = y-X*βhat       \nprintln(\"Test residuals : \",@test res ≈ residuals(lm1))\n\n# estimation of σ²\nσ²hat = sum(res.^2)/(n-2)  \nσhat = sqrt(σ²hat)\nprintln(\"σ²hat = \", σ²hat)\n\n\nprintln(\"σ²hat = \", σ²hat)\n\n\n# Variances, covariances matrix of B\nΣ = σ²hat*inv(X'*X)      \nprintln(\"Σ = \", Σ)\ncovB = vcov(lm1)       # GLM Package\nprintln(\"Test Variance, covariance matrix : \", @test Σ ≈ covB)\n\nusing Distributions\nusing HypothesisTests\nα = 0.05\n\nt_crit = quantile(TDist(n-2),1-α/2)\n\nσhat₀ = sqrt(Σ[1,1])\nσhat₁ = sqrt(Σ[2,2])\nprintln(\"test of the variance of σhat₀ : \", @test σhat₀ ≈ stderror(lm1)[1])\nprintln(\"test of the variance of σhat₁ : \", @test σhat₁ ≈ stderror(lm1)[2])\n\nprintln(\"β₀ ∈ [\", βhat₀ - t_crit*σhat₀, \",\",βhat₀ + t_crit*σhat₀,\"]\")\nprintln(\"β₁ ∈ [\", βhat₁ - t_crit*σhat₁, \",\",βhat₁ + t_crit*σhat₁,\"]\")\n\nTest residuals : Test Passed\nσ²hat = 0.10866889312977099\nσ²hat = 0.10866889312977099\nΣ = [0.03609767024885643 -0.0007491724741055301; -0.0007491724741055301 1.710912916642387e-5]\nTest Variance, covariance matrix : Test Passed\ntest of the variance of σhat₀ : Test Passed\ntest of the variance of σhat₁ : Test Passed\nβ₀ ∈ [1.2520803311678832,2.027070432190896]\nβ₁ ∈ [0.016473274332038396,0.033345427958037924]\n\n\n\n\n\n\nThe condidence band for the mean is \\hat{\\beta}_0 +\\hat{\\beta}_1x \\pm t_{1-\\alpha/2}\\hat{\\sigma}\\sqrt{\\frac{1}{n}+\\frac{(x-\\bar{x})^2}{SCE_x}}\nThe Prediction band is \\hat{\\beta}_0 +\\hat{\\beta}_1x \\pm t_{1-\\alpha/2}\\hat{\\sigma}\\sqrt{1+\\frac{1}{n}+\\frac{(x-\\bar{x})^2}{SCE_x}}\n\n\n\n\nPlot these confidence and prevision bands on the graph p\n\nprintln(\"n= \", n)\nprintln(\"x = \", x)\n\n\nt_crit = quantile(TDist(n-2),1-α/2)\nxbar = mean(x)\nSCE = sum((x .- xbar).^2)\nxx = 20:1:70\np = plot(x,y, seriestype = :scatter, xlabel = (L\"x\"), ylabel = (L\"y\"), label=\"\")\nplot!(p,xx,βhat₀ .+ βhat₁*xx, label=\"\")\n\n# Confidence Bands\n\nyy_sup = βhat₀ .+ βhat₁*xx + t_crit*σhat*sqrt.(1/n .+ (xx .-xbar).^2/SCE)\nplot!(p, xx, yy_sup, c=:green, label = \"Confidence bands\")\nyy_inf = βhat₀ .+ βhat₁*xx - t_crit*σhat*sqrt.(1/n .+ (xx .-xbar).^2/SCE)\nplot!(p, xx, yy_inf, c=:green, label = \"\")\n\n# Prevision Bands\nxx = 20:1:70\nyy_sup = βhat₀ .+ βhat₁*xx + t_crit*σhat*sqrt.(1 .+ 1/n .+ (xx .-xbar).^2/SCE)\nplot!(p, xx, yy_sup, c=:blue, label = \"Prevision bands\")\nyy_inf = βhat₀ .+ βhat₁*xx - t_crit*σhat*sqrt.(1 .+ 1/n .+ (xx .-xbar).^2/SCE)\nplot!(p, xx, yy_inf, c=:blue, label = \"\")\n\nn= 33\nx = [25, 25, 25, 25, 25, 25, 25, 35, 35, 35, 35, 35, 35, 35, 45, 45, 45, 45, 45, 45, 45, 45, 55, 55, 55, 55, 55, 65, 65, 65, 65, 65, 65]"
  },
  {
    "objectID": "M2/resources/TP2-linear-regression.html#parameters-estimation-by-least-square",
    "href": "M2/resources/TP2-linear-regression.html#parameters-estimation-by-least-square",
    "title": "TP2 : Linear regression",
    "section": "",
    "text": "We suppose that for i=1,\\ldots,n Y_i=\\beta_0+\\beta_1 x+\\varepsilon_i\nwith - E(\\varepsilon_i)=0 - The (\\varepsilon_i)_{i=1,\\dots,n} are i.i.d. - The distributions of (\\varepsilon_i)_{i=1,\\dots,n} are \\mathcal{N}(0,\\sigma^2)\n\n\n\nWe use the mean square estimation of the parameters \\beta_0 and \\beta_1:\n\\begin{array}{ll}\nMin\\quad\nf(\\beta)=\\frac{1}{2}\\sum_{i=1}^{n}r_{i}^{2}(\\beta)& =\\frac{1}{2}\\|r(\\beta)\\|^2\\\\\n\\beta \\in \\mathbf{R}^{2}&\n\\end{array}\nwhere r is the residual function r:\\mathbf{R}^2\\rightarrow \\mathbf{R}^n r_i(\\beta) =  y_i - \\beta_{0}+\\beta_{1}x_{i}\nSo we can write r(\\beta) = y - X\\beta, where\ny = \\begin{pmatrix} y_1\\\\ \\vdots \\\\ y_n\\end{pmatrix}\\;\\,\\texttt{and}\\;\\;\nX = \\begin{pmatrix} 1 & x_1\\\\ \\vdots & \\vdots \\\\ 1 & x_n\\end{pmatrix}\n\nusing Test \nusing DataFrames\nusing Plots\nusing LaTeXStrings\nusing Statistics\nusing DataFrames\nusing GLM\n\ny=[1.8, 2.3, 2, 2.4, 2, 2.5, 2.6, 2.6, 2.9, 2.3, 2.4, 2.1, 2.5, 2.7, 2.7, 3, 3.1, 2.3, 2.5, 3, 3.3, 2.7, 3.1, 2.9, 3.4, 2.4, 3.4, 3.7, 2.8, 3.3, 3.5, 3.3, 2.6]\nx=[25, 25, 25, 25, 25, 25, 25, 35, 35, 35, 35, 35, 35, 35, 45, 45, 45, 45, 45, 45, 45, 45, 55, 55, 55, 55, 55, 65, 65, 65, 65, 65, 65]\nn = length(x)\n\np = plot(x,y, seriestype = :scatter, xlabel = (L\"x\"), ylabel = (L\"y\"))\na , b = .5 , 0.04\nplot!(p,x,a .+ b*x)\n\nr(a,b) = y - (a .+ b*x)\n\nr(a,b)\n\ni = 11\nplot!(p, [x[i],x[i]], [y[i],a+b*x[i]],linewidth=3)\nannotate!(p,x[i],mean([y[i],a+b*x[i]]),text(L\"r_{11}(a,b)\", :left, :green))\nplot(p,legend=false)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor solving the optimization problem we have to sole the linear system X^TX\\beta = X^Ty\nUse the  operator for solving linear equation\n\n# \n# Use of the GLM Package\ndatas = DataFrame(age=x, chol = y)\nlm1 = lm(@formula(chol ~ age), datas)\nprintln(coef(lm1))\nprintln(lm1)\n\n# ... To complete\nX = [ones(n) x]\n\nβhat₀, βhat₁ = (X'*X)\\(X'*y)\nβhat = [βhat₀, βhat₁]\nprintln(\"Normal equation : βhat₀, βhat₁ = \", βhat₀, \", \", βhat₁)\nβhat₀, βhat₁ = X\\y\nprintln(\"\\\\ of julia : βhat₀, βhat₁ = \", βhat₀, \", \", βhat₁)\np = plot(x,y, seriestype = :scatter, xlabel = (L\"x\"), ylabel = (L\"y\"))\nplot!(p,x,βhat₀ .+ βhat₁*x)\n\nprintln(@test βhat ≈ coef(lm1))\n\n[1.639575381679391, 0.024909351145038124]\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n\nchol ~ 1 + age\n\nCoefficients:\n────────────────────────────────────────────────────────────────────────\n                 Coef.  Std. Error     t  Pr(&gt;|t|)  Lower 95%  Upper 95%\n────────────────────────────────────────────────────────────────────────\n(Intercept)  1.63958    0.189994    8.63    &lt;1e-09  1.25208    2.02707\nage          0.0249094  0.00413632  6.02    &lt;1e-05  0.0164733  0.0333454\n────────────────────────────────────────────────────────────────────────\nNormal equation : βhat₀, βhat₁ = 1.6395753816793879, 0.0249093511450382\n\\ of julia : βhat₀, βhat₁ = 1.6395753816793897, 0.02490935114503816\nTest Passed\n\n\n\n\n\n\nWe note r_i = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i), then the unbiased estimation of the variance \\sigma^2 is\n\n\\hat{\\sigma}^2\\frac{\\sum_ir_i^2}{n-2}\n\nAs \\hat{\\beta} = (X^TX)^{-1}X^Ty the statistic B=(X^TX)^{-1}X^TY = (X^TX)^{-1}X^T(X\\beta+\\varepsilon)=\\beta +(X^TX)^{-1}X^T\\varepsilon as the \\mathcal{N}(\\beta,\\sigma^2(X^TX)^{-1}) distribution.\n\nSi les postulats sont vérifiés alors les intervalles de confiances de \\beta _{0},\\beta _{1} et \\sigma ^{2} au seuil (1-\\alpha ) sont:\n\n\n\\sigma ^{2}\\in \\left[ \\frac{\\sum_{i=1}^{n}r_{i}^{2}}{\\chi _{1-\\alpha /2}};%\n\\frac{\\sum_{i=1}^{n}r_{i}^{2}}{\\chi _{\\alpha /2}}\\right]\n\n\nwhere \\chi _{1-\\alpha /2} and \\chi _{\\alpha /2} sont lues dans le table du Khi-2 à \\nu =n-2 ddl.\n\n\\begin{array}{ccc}\n\\beta _{1} & \\in  & [\\hat{\\beta}_{1}-t_{1-\\alpha /2}\\hat{\\sigma}%\n_{B_{1}}^{{}};\\hat{\\beta}_{1}+t_{1-\\alpha /2}\\hat{\\sigma}_{B_{1}}^{{}}] \\\\\n\\beta _{0} & \\in  & [\\hat{\\beta}_{0}-t_{1-\\alpha /2}\\hat{\\sigma}%\n_{B_{0}}^{{}};\\hat{\\beta}_{0}+t_{1-\\alpha /2}\\hat{\\sigma}_{B_{0}}^{{}}]\n\\end{array}\n\noù - ${B{1}}^{2} = $ - \\hat{\\sigma}_{B_{0}}^{2} = \\hat{\\sigma}^{2}(\\dfrac{1}{n}+\\dfrac{\\bar{x}^{2}}{SCE_{x}}) - t_{1-\\alpha /2} from the Student law with n-2 degrees of freedom\n\n\n\n\nCompute the n residuals in the vector res\nCompute the estimation of residual variance \\sigma^2 and of the standard error \\sigma\nCompute the estimation of the variance, covariance matrix \\Sigma.\nCompute the estimation of the standard error of \\hat{\\beta}_0 and \\hat{\\beta}_1.\nCompute the confidence intervalle of \\beta_0 and \\beta_1\n\n\n# Compute the residuals\nres = y-X*βhat       \nprintln(\"Test residuals : \",@test res ≈ residuals(lm1))\n\n# estimation of σ²\nσ²hat = sum(res.^2)/(n-2)  \nσhat = sqrt(σ²hat)\nprintln(\"σ²hat = \", σ²hat)\n\n\nprintln(\"σ²hat = \", σ²hat)\n\n\n# Variances, covariances matrix of B\nΣ = σ²hat*inv(X'*X)      \nprintln(\"Σ = \", Σ)\ncovB = vcov(lm1)       # GLM Package\nprintln(\"Test Variance, covariance matrix : \", @test Σ ≈ covB)\n\nusing Distributions\nusing HypothesisTests\nα = 0.05\n\nt_crit = quantile(TDist(n-2),1-α/2)\n\nσhat₀ = sqrt(Σ[1,1])\nσhat₁ = sqrt(Σ[2,2])\nprintln(\"test of the variance of σhat₀ : \", @test σhat₀ ≈ stderror(lm1)[1])\nprintln(\"test of the variance of σhat₁ : \", @test σhat₁ ≈ stderror(lm1)[2])\n\nprintln(\"β₀ ∈ [\", βhat₀ - t_crit*σhat₀, \",\",βhat₀ + t_crit*σhat₀,\"]\")\nprintln(\"β₁ ∈ [\", βhat₁ - t_crit*σhat₁, \",\",βhat₁ + t_crit*σhat₁,\"]\")\n\nTest residuals : Test Passed\nσ²hat = 0.10866889312977099\nσ²hat = 0.10866889312977099\nΣ = [0.03609767024885643 -0.0007491724741055301; -0.0007491724741055301 1.710912916642387e-5]\nTest Variance, covariance matrix : Test Passed\ntest of the variance of σhat₀ : Test Passed\ntest of the variance of σhat₁ : Test Passed\nβ₀ ∈ [1.2520803311678832,2.027070432190896]\nβ₁ ∈ [0.016473274332038396,0.033345427958037924]\n\n\n\n\n\n\nThe condidence band for the mean is \\hat{\\beta}_0 +\\hat{\\beta}_1x \\pm t_{1-\\alpha/2}\\hat{\\sigma}\\sqrt{\\frac{1}{n}+\\frac{(x-\\bar{x})^2}{SCE_x}}\nThe Prediction band is \\hat{\\beta}_0 +\\hat{\\beta}_1x \\pm t_{1-\\alpha/2}\\hat{\\sigma}\\sqrt{1+\\frac{1}{n}+\\frac{(x-\\bar{x})^2}{SCE_x}}\n\n\n\n\nPlot these confidence and prevision bands on the graph p\n\nprintln(\"n= \", n)\nprintln(\"x = \", x)\n\n\nt_crit = quantile(TDist(n-2),1-α/2)\nxbar = mean(x)\nSCE = sum((x .- xbar).^2)\nxx = 20:1:70\np = plot(x,y, seriestype = :scatter, xlabel = (L\"x\"), ylabel = (L\"y\"), label=\"\")\nplot!(p,xx,βhat₀ .+ βhat₁*xx, label=\"\")\n\n# Confidence Bands\n\nyy_sup = βhat₀ .+ βhat₁*xx + t_crit*σhat*sqrt.(1/n .+ (xx .-xbar).^2/SCE)\nplot!(p, xx, yy_sup, c=:green, label = \"Confidence bands\")\nyy_inf = βhat₀ .+ βhat₁*xx - t_crit*σhat*sqrt.(1/n .+ (xx .-xbar).^2/SCE)\nplot!(p, xx, yy_inf, c=:green, label = \"\")\n\n# Prevision Bands\nxx = 20:1:70\nyy_sup = βhat₀ .+ βhat₁*xx + t_crit*σhat*sqrt.(1 .+ 1/n .+ (xx .-xbar).^2/SCE)\nplot!(p, xx, yy_sup, c=:blue, label = \"Prevision bands\")\nyy_inf = βhat₀ .+ βhat₁*xx - t_crit*σhat*sqrt.(1 .+ 1/n .+ (xx .-xbar).^2/SCE)\nplot!(p, xx, yy_inf, c=:blue, label = \"\")\n\nn= 33\nx = [25, 25, 25, 25, 25, 25, 25, 35, 35, 35, 35, 35, 35, 35, 45, 45, 45, 45, 45, 45, 45, 45, 55, 55, 55, 55, 55, 65, 65, 65, 65, 65, 65]"
  },
  {
    "objectID": "M2/resources/TP2-linear-regression.html#simulation-of-the-distribution-the-estimates-of-beta_0-and-beta_1",
    "href": "M2/resources/TP2-linear-regression.html#simulation-of-the-distribution-the-estimates-of-beta_0-and-beta_1",
    "title": "TP2 : Linear regression",
    "section": "Simulation of the distribution the estimates of \\beta_0 and \\beta_1",
    "text": "Simulation of the distribution the estimates of \\beta_0 and \\beta_1\nWe consider the model Y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i with - (\\varepsilon_i)_{i=1,\\ldots,n} i.i.d. - The \\varepsilon_i have the \\mathcal{N}(0,\\sigma^2) distribution - (x_1,\\ldots,x_n) = (1,2,\\ldots,10) - \\beta_0 = 2.0, \\beta_1 = 1.5 and \\sigma = 1.5\nWe recall that the statistic B which estimate the parameter \\beta follows the \\mathcal{N}(\\beta,\\Sigma) distribution, with \\Sigma=\\sigma^2(X^TX)^{-1}.\n\nExercise (more difficult)\n\nDefine a function coefEst() with return the estimation of the parameters \\beta_0 and \\beta_1. We generate a sample of size n=10 of the (y_i)_{i=1,\\ldots,n} inside the fonction\nCompute N samples of the estimation \\hat{\\beta}\nCompute \\Sigma^{-1}\n\n\n\nusing LinearAlgebra\n# initialisation for generating the datas\nβ₀, β₁ = 2.0, 1.5\nβ = [β₀, β₁]\nσ = 2.5\nn, N = 10, 10^4     # n = size of samples, N = number of samples\nα = 0.05\n\nxVals = collect(1:n) # return the vector [1,2,...,n]\nX = [ones(n) xVals]\nfunction  coefEst()\n    \"\"\" the function first generate the y1,... , yn \n    The values of x and y are then set in a DataFrame, and the linear model is created\n    We return the value in a 2 dimensional vector coeff(model)\n    \"\"\"\n    yVals = β₀ .+ β₁*xVals +rand(Normal(0,σ),n)   # genetating a sample\n    data = DataFrame([xVals,yVals], [:x, :y])     # create a DataFrame from the datas\n    model = lm(@formula(y ~ x), data)             # linear regresseion\n    return (coef(model))                          # return the estimation of β₀ and β₁\nend\n\nests = [coefEst() for _ in  1:N]    # generating N sample of the parameters (in an array)\n\n# The statistic B which estimate β follows the N(β,Σ) distribution with, Σ = σ²(XᵀX)⁻¹\nΣ⁻¹ = (1/σ^2)*X'*X    # Compute the inverse of the variance, covariance matrix \nA = cholesky(Σ⁻¹).U   # Z = A(B-β) follows the N(0,I) distribution\nA⁻¹ = inv(A)\n# The radius of a standard bidimensional normal distribution follows a Rayleigh law\nradius = quantile(Rayleigh(),1-α)  # Z₁² + Z₂² follows a χ^2 with 2 dof distribution\nisInEllipse(b) = norm(A*(b-β)) &lt;= radius # function which indicate if b is in the ellipse or not.\n                                         # A*(b-β) transform an ellipse to a circle\nestIn = isInEllipse.(ests)       # Vectorize the function\n\nprintln(\"Percentage of points inside the ellipse : \", sum(estIn)/N)\nscatter(first.(ests[estIn]), last.(ests[estIn]), color = :green, ms=2, msw=0) # display points inside the ellipse \nscatter!(first.(ests[.!estIn]), last.(ests[.!estIn]), color = :red, ms=2, msw=0) # display points outside the ellipse\n# Calculus of the ellipse\nellipsePts = [radius*A⁻¹*[cos(t),sin(t)] + β for t in 0:0.01:2*pi] # compute the ellipse\nscatter!([β₀], [β₁], c=:red, ms=5, msw=2)   # display the center of the ellipse\nplot!(first.(ellipsePts), last.(ellipsePts), c=:blue, lw=2, legend=false,xlabel=L\"\\hat{\\beta}_0\", ylabel=L\"\\hat{\\beta}_1\")\n\nPercentage of points inside the ellipse : 0.9495"
  },
  {
    "objectID": "M2/index.html",
    "href": "M2/index.html",
    "title": "Julia Master 2 course",
    "section": "",
    "text": "This part is in construction.\n\n\n\n\n\nExercices\n\nIntroduction to Julia for statistics through practice\n\n\n\nDistribution of random variables\nLinear regression\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Master 2"
    ]
  }
]